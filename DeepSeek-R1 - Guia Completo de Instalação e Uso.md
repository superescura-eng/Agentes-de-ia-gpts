# üìö DeepSeek-R1 - Guia Completo de Instala√ß√£o e Uso

> **Vers√£o:** 2.0  
> **Data:** Dezembro 2024  
> **Licen√ßa:** MIT (uso comercial permitido)

---

## üìã √çndice

1. [Introdu√ß√£o](#-introdu√ß√£o)
2. [Arquitetura DeepSeek-V3](#-arquitetura-deepseek-v3)
3. [Modelos Dispon√≠veis](#-modelos-dispon√≠veis)
4. [Requisitos de Sistema](#-requisitos-de-sistema)
5. [M√©todos de Instala√ß√£o](#-m√©todos-de-instala√ß√£o)
   - [Usando Ollama (Recomendado para Iniciantes)](#1-usando-ollama-recomendado-para-iniciantes)
   - [Usando vLLM (Recomendado para Produ√ß√£o)](#2-usando-vllm-recomendado-para-produ√ß√£o)
   - [Usando SGLang](#3-usando-sglang)
   - [Usando LMDeploy](#4-usando-lmdeploy)
   - [Usando TensorRT-LLM](#5-usando-tensorrt-llm)
   - [Via API DeepSeek](#6-via-api-deepseek)
6. [Uso Online (Sem Instala√ß√£o)](#-uso-online-sem-instala√ß√£o)
7. [Configura√ß√µes Recomendadas](#-configura√ß√µes-recomendadas)
8. [Exemplos de Uso](#-exemplos-de-uso)
9. [Pre√ßos da API](#-pre√ßos-da-api)
10. [Troubleshooting](#-troubleshooting)
11. [Recursos Adicionais](#-recursos-adicionais)

---

## üéØ Introdu√ß√£o

O **DeepSeek-R1** √© um modelo de racioc√≠nio de primeira gera√ß√£o desenvolvido pela DeepSeek-AI, uma empresa chinesa de intelig√™ncia artificial. O modelo foi treinado usando **Reinforcement Learning (RL) em larga escala** e demonstra desempenho compar√°vel ao OpenAI-o1 em tarefas de matem√°tica, c√≥digo e racioc√≠nio.

### Caracter√≠sticas Principais

| Caracter√≠stica | Descri√ß√£o |
|----------------|-----------|
| üß† **Racioc√≠nio Avan√ßado** | Capacidade de auto-verifica√ß√£o, reflex√£o e gera√ß√£o de cadeias de pensamento longas (CoT) |
| üîì **Open Source** | C√≥digo e pesos do modelo licenciados sob MIT |
| üíº **Uso Comercial** | Permitido para modifica√ß√µes, trabalhos derivados e destila√ß√£o |
| üéì **Destila√ß√£o** | Modelos menores treinados com dados gerados pelo R1 |

### Variantes do Modelo

1. **DeepSeek-R1-Zero** - Treinado apenas com RL, sem fine-tuning supervisionado (SFT)
   - Primeiro modelo open-source a validar que capacidades de racioc√≠nio podem ser incentivadas puramente atrav√©s de RL
   - Demonstra auto-verifica√ß√£o, reflex√£o e gera√ß√£o de CoTs longos
   
2. **DeepSeek-R1** - Vers√£o aprimorada com cold-start data antes do RL
   - Pipeline com 2 est√°gios de RL + 2 est√°gios de SFT
   - Resolve problemas de repeti√ß√£o infinita e mistura de idiomas do R1-Zero
   
3. **DeepSeek-R1-Distill** - Modelos menores destilados baseados em Qwen e Llama
   - Treinados com 800k amostras geradas pelo DeepSeek-R1
   - O modelo 32B supera o OpenAI-o1-mini em benchmarks!

---

## üèóÔ∏è Arquitetura DeepSeek-V3

O DeepSeek-R1 √© baseado no **DeepSeek-V3**, um modelo Mixture-of-Experts (MoE) de √∫ltima gera√ß√£o:

### Especifica√ß√µes T√©cnicas

| Especifica√ß√£o | Valor |
|---------------|-------|
| **Par√¢metros Totais** | 671B |
| **Par√¢metros Ativos por Token** | 37B |
| **Tokens de Pr√©-treinamento** | 14.8 Trilh√µes |
| **Horas de GPU (Treinamento)** | 2.788M H800 GPU hours |
| **Arquitetura de Aten√ß√£o** | Multi-head Latent Attention (MLA) |

### Inova√ß√µes Arquiteturais

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DeepSeek-V3 Architecture                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Multi-head Latent Attention (MLA)                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Gest√£o eficiente de mem√≥ria de aten√ß√£o               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Validado no DeepSeek-V2                              ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                              ‚îÇ                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  DeepSeekMoE (Mixture of Experts)                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - 671B par√¢metros totais                               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Apenas 37B ativados por token                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Estrat√©gia de balanceamento sem loss auxiliar        ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                              ‚îÇ                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Multi-Token Prediction (MTP)                           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Objetivo de treinamento inovador                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Permite speculative decoding na infer√™ncia           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - M√≥dulo MTP: 14B par√¢metros adicionais                ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                              ‚îÇ                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  FP8 Mixed Precision Training                           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Primeira valida√ß√£o em modelo de escala extrema       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Supera√ß√£o de gargalos de comunica√ß√£o cross-node      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Overlap quase total de computa√ß√£o-comunica√ß√£o        ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### P√≥s-Treinamento com DeepSeek-R1

O DeepSeek-V3 incorpora destila√ß√£o de conhecimento do DeepSeek-R1:
- Integra padr√µes de **verifica√ß√£o** e **reflex√£o** do R1
- Melhora significativamente o desempenho em racioc√≠nio
- Mant√©m controle sobre estilo e comprimento das sa√≠das

---

## üì¶ Modelos Dispon√≠veis

### Modelos Principais (Baseados no DeepSeek-V3)

| Modelo | Par√¢metros | Tipo | Download |
|--------|------------|------|----------|
| DeepSeek-R1-Zero | ~671B | Base (MoE) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero) |
| DeepSeek-R1 | ~671B | Chat (MoE) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1) |
| DeepSeek-V3-Base | 671B + 14B MTP | Base | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base) |
| DeepSeek-V3 | 671B + 14B MTP | Chat | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3) |

> ‚ö†Ô∏è **Nota:** O tamanho total no HuggingFace √© 685B (671B Main Model + 14B MTP Module)

### Modelos Destilados (Recomendados para Uso Local)

| Modelo | Par√¢metros | Base | Tamanho Aprox. | Download |
|--------|------------|------|----------------|----------|
| DeepSeek-R1-Distill-Qwen-1.5B | 1.5B | Qwen2.5-Math | ~1.1 GB | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) |
| DeepSeek-R1-Distill-Qwen-7B | 7B | Qwen2.5-Math | ~4.7 GB | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) |
| DeepSeek-R1-Distill-Llama-8B | 8B | Llama-3.1 | ~5 GB | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B) |
| DeepSeek-R1-Distill-Qwen-14B | 14B | Qwen2.5 | ~9 GB | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B) |
| DeepSeek-R1-Distill-Qwen-32B | 32B | Qwen2.5 | ~20 GB | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) |
| DeepSeek-R1-Distill-Llama-70B | 70B | Llama-3.3 | ~40 GB | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B) |

> üí° **Destaque:** O modelo `DeepSeek-R1-Distill-Qwen-32B` supera o OpenAI-o1-mini em diversos benchmarks, atingindo novos resultados state-of-the-art para modelos densos!

---

## üíª Requisitos de Sistema

### Requisitos M√≠nimos por Modelo

| Modelo | RAM | VRAM GPU | Armazenamento |
|--------|-----|----------|---------------|
| 1.5B | 8 GB | 4 GB | 5 GB |
| 7B | 16 GB | 8 GB | 10 GB |
| 8B | 16 GB | 10 GB | 12 GB |
| 14B | 32 GB | 16 GB | 20 GB |
| 32B | 64 GB | 24-48 GB | 40 GB |
| 70B | 128 GB | 80+ GB | 80 GB |
| 671B (Full) | 512+ GB | Multi-GPU (8x H100/A100) | 1.5 TB |

### Software Necess√°rio

#### Para vLLM (Recomendado para Produ√ß√£o)
- **Sistema Operacional:** Linux (Windows n√£o suportado diretamente)
- **Python:** 3.10 - 3.13
- **CUDA:** 11.8+ (para GPU NVIDIA)
- **ROCm:** 6.0+ (para GPU AMD)

#### Para Ollama (Mais F√°cil)
- **Sistema Operacional:** Windows 10/11, Linux, macOS
- **RAM:** M√≠nimo 8GB (recomendado 16GB+)

---

## üîß M√©todos de Instala√ß√£o

### 1. Usando Ollama (Recomendado para Iniciantes)

O **Ollama** √© a forma mais f√°cil de rodar o DeepSeek-R1 localmente. Ele gerencia downloads, quantiza√ß√£o e execu√ß√£o automaticamente.

#### Passo 1: Instalar o Ollama

**Windows:**
```powershell
# Baixe o instalador em: https://ollama.com/download
# Ou use winget:
winget install Ollama.Ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**macOS:**
```bash
# Via Homebrew
brew install ollama
# Ou baixe em: https://ollama.com/download
```

#### Passo 2: Verificar Instala√ß√£o

```bash
ollama --version
```

#### Passo 3: Baixar e Executar o DeepSeek-R1

```bash
# Modelo 1.5B (mais leve - ideal para testes)
ollama run deepseek-r1:1.5b

# Modelo 7B (equilibrado - recomendado)
ollama run deepseek-r1:7b

# Modelo 8B
ollama run deepseek-r1:8b

# Modelo 14B
ollama run deepseek-r1:14b

# Modelo 32B (alta performance)
ollama run deepseek-r1:32b

# Modelo 70B (melhor qualidade)
ollama run deepseek-r1:70b
```

> ‚ö†Ô∏è **Nota:** Na primeira execu√ß√£o, o modelo ser√° baixado automaticamente. Isso pode levar alguns minutos dependendo da sua conex√£o.

#### Comandos √öteis do Ollama

```bash
# Listar modelos instalados
ollama list

# Remover um modelo
ollama rm deepseek-r1:7b

# Mostrar informa√ß√µes do modelo
ollama show deepseek-r1:7b

# Rodar modelo em modo servidor (API)
ollama serve

# Usar API via curl
curl http://localhost:11434/api/generate -d '{
  "model": "deepseek-r1:7b",
  "prompt": "Explique a teoria da relatividade"
}'
```

---

### 2. Usando vLLM (Recomendado para Produ√ß√£o)

O **vLLM** √© uma biblioteca de alta performance para servir LLMs, desenvolvida originalmente no Sky Computing Lab da UC Berkeley. Agora √© um projeto da PyTorch Foundation!

#### Caracter√≠sticas do vLLM

| Feature | Descri√ß√£o |
|---------|-----------|
| **PagedAttention** | Gest√£o eficiente de mem√≥ria de aten√ß√£o |
| **Continuous Batching** | Batching cont√≠nuo de requisi√ß√µes |
| **CUDA/HIP Graph** | Execu√ß√£o r√°pida de modelos |
| **Quantiza√ß√£o** | GPTQ, AWQ, AutoRound, INT4, INT8, FP8 |
| **Speculative Decoding** | Decodifica√ß√£o especulativa para acelerar |
| **Chunked Prefill** | Prefill em chunks para otimiza√ß√£o |
| **API OpenAI-Compatible** | Drop-in replacement para OpenAI API |
| **Multi-Hardware** | NVIDIA, AMD, Intel, TPU, Arm, Huawei Ascend |

#### Pr√©-requisitos

- Linux (obrigat√≥rio)
- Python 3.10 - 3.13
- GPU NVIDIA com CUDA 11.8+ ou AMD com ROCm

#### Instala√ß√£o do vLLM

**M√©todo 1: Usando uv (Recomendado)**
```bash
# Instalar uv (gerenciador de ambiente Python ultra-r√°pido)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Criar ambiente e instalar vLLM
uv venv --python 3.12 --seed
source .venv/bin/activate
uv pip install vllm --torch-backend=auto
```

**M√©todo 2: Usando conda + pip**
```bash
conda create -n vllm-env python=3.12 -y
conda activate vllm-env
pip install --upgrade uv
uv pip install vllm --torch-backend=auto
```

**M√©todo 3: Via pip direto**
```bash
pip install vllm
```

**M√©todo 4: Docker (AMD GPUs)**
```bash
docker pull rocm/vllm:latest
docker run --device=/dev/kfd --device=/dev/dri \
  -v <path/to/your/models>:/app/models \
  -it rocm/vllm:latest
```

#### Servir Modelos DeepSeek-R1-Distill

```bash
# Modelo 7B (GPU √∫nica)
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \
    --max-model-len 16384

# Modelo 32B (2 GPUs com tensor parallelism)
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \
    --tensor-parallel-size 2 \
    --max-model-len 32768 \
    --enforce-eager

# Modelo 70B (m√∫ltiplas GPUs)
vllm serve deepseek-ai/DeepSeek-R1-Distill-Llama-70B \
    --tensor-parallel-size 4 \
    --max-model-len 32768
```

#### Servir DeepSeek-V3/R1 Completo

Para o modelo completo de 671B, √© necess√°rio multi-node:

```bash
# Requer cluster com m√∫ltiplas m√°quinas
# Consulte: https://docs.vllm.ai/en/latest/serving/distributed_serving.html
vllm serve deepseek-ai/DeepSeek-V3 \
    --pipeline-parallel-size 8 \
    --tensor-parallel-size 8
```

#### Usar API OpenAI-Compatible

O servidor vLLM exp√µe uma API compat√≠vel com OpenAI em `http://localhost:8000`:

```python
# Instalar: pip install openai
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="empty"  # vLLM n√£o requer API key por padr√£o
)

# Chat Completion
response = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    messages=[
        {"role": "user", "content": "Resolva: 2x + 5 = 15"}
    ],
    temperature=0.6
)

print(response.choices[0].message.content)
```

**Listar modelos dispon√≠veis:**
```bash
curl http://localhost:8000/v1/models
```

#### Infer√™ncia Offline em Batch

```python
from vllm import LLM, SamplingParams

# Inicializar modelo
llm = LLM(model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")

# Definir prompts
prompts = [
    "Qual √© a capital do Brasil?",
    "Explique a teoria da evolu√ß√£o.",
    "Resolva: x¬≤ - 4 = 0"
]

# Configurar par√¢metros de sampling
sampling_params = SamplingParams(
    temperature=0.6,
    top_p=0.95,
    max_tokens=1024
)

# Gerar
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Resposta: {output.outputs[0].text}\n")
```

---

### 3. Usando SGLang

**SGLang** oferece otimiza√ß√µes state-of-the-art para DeepSeek, incluindo MLA optimizations, DP Attention, FP8 e Torch Compile.

#### Instala√ß√£o

```bash
pip install sglang
```

#### Iniciar Servidor

```bash
# Modelo 32B com 2 GPUs
python3 -m sglang.launch_server \
    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \
    --trust-remote-code \
    --tp 2

# DeepSeek-V3 completo (multi-node)
# Consulte: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3
```

#### Recursos do SGLang

- ‚úÖ MLA Optimizations
- ‚úÖ DP Attention para modelos DeepSeek
- ‚úÖ FP8 (W8A8)
- ‚úÖ FP8 KV Cache
- ‚úÖ Torch Compile
- ‚úÖ Multi-node tensor parallelism
- ‚úÖ Suporte NVIDIA e AMD GPUs
- üîÑ Multi-Token Prediction (em desenvolvimento)

---

### 4. Usando LMDeploy

**LMDeploy** √© um framework flex√≠vel e de alta performance para servir LLMs.

#### Instala√ß√£o

```bash
pip install lmdeploy
```

#### Uso

Consulte a documenta√ß√£o oficial: [LMDeploy DeepSeek-V3 Guide](https://github.com/InternLM/lmdeploy/issues/2960)

---

### 5. Usando TensorRT-LLM

**TensorRT-LLM** da NVIDIA oferece suporte ao DeepSeek-V3 com op√ß√µes de precis√£o BF16 e INT4/INT8.

#### Recursos

- ‚úÖ BF16
- ‚úÖ INT4/INT8 weight-only
- üîÑ FP8 (em desenvolvimento)

#### Guia

Consulte: [TensorRT-LLM DeepSeek-V3](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3)

---

### 6. Via API DeepSeek

Se voc√™ n√£o quer instalar nada localmente, use a API oficial do DeepSeek (compat√≠vel com OpenAI!).

#### Configura√ß√£o

1. Acesse [platform.deepseek.com](https://platform.deepseek.com/)
2. Crie uma conta e obtenha sua [API Key](https://platform.deepseek.com/api_keys)
3. Use a API (100% compat√≠vel com OpenAI SDK)

#### Endpoints Dispon√≠veis

| Endpoint | URL |
|----------|-----|
| Base URL | `https://api.deepseek.com` |
| Alternativa (v1) | `https://api.deepseek.com/v1` |

> ‚ö†Ô∏è **Nota:** O `/v1` na URL N√ÉO tem rela√ß√£o com a vers√£o do modelo!

#### Modelos via API

| Modelo | Descri√ß√£o |
|--------|-----------|
| `deepseek-chat` | Modo n√£o-thinking do DeepSeek-V3.2 |
| `deepseek-reasoner` | Modo thinking do DeepSeek-V3.2 (equivalente ao R1) |

#### Exemplos de Uso

**Python (OpenAI SDK):**
```python
# pip install openai
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.environ.get('DEEPSEEK_API_KEY'),
    base_url="https://api.deepseek.com"
)

response = client.chat.completions.create(
    model="deepseek-chat",  # ou "deepseek-reasoner" para modo thinking
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"}
    ],
    stream=False
)

print(response.choices[0].message.content)
```

**Node.js:**
```javascript
// npm install openai
import OpenAI from "openai";

const openai = new OpenAI({
    baseURL: 'https://api.deepseek.com',
    apiKey: process.env.DEEPSEEK_API_KEY,
});

async function main() {
    const completion = await openai.chat.completions.create({
        messages: [{ role: "system", content: "You are a helpful assistant." }],
        model: "deepseek-chat",
    });
    console.log(completion.choices[0].message.content);
}

main();
```

**cURL:**
```bash
curl https://api.deepseek.com/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${DEEPSEEK_API_KEY}" \
  -d '{
    "model": "deepseek-chat",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ],
    "stream": false
  }'
```

#### Funcionalidades da API

| Recurso | Descri√ß√£o |
|---------|-----------|
| [JSON Output](https://api-docs.deepseek.com/guides/json_mode) | For√ßar sa√≠da em formato JSON |
| [Tool Calls](https://api-docs.deepseek.com/guides/tool_calls) | Function calling |
| [Chat Prefix Completion](https://api-docs.deepseek.com/guides/chat_prefix_completion) | Completar a partir de prefixo (Beta) |
| [FIM Completion](https://api-docs.deepseek.com/guides/fim_completion) | Fill-in-the-middle (Beta) |

---

## üåê Uso Online (Sem Instala√ß√£o)

Se voc√™ quer testar o modelo rapidamente sem instalar nada:

1. Acesse [chat.deepseek.com](https://chat.deepseek.com)
2. Crie uma conta gratuita
3. Ative o bot√£o **"DeepThink"** para usar o modo de racioc√≠nio avan√ßado (R1)

---

## ‚öôÔ∏è Configura√ß√µes Recomendadas

Para obter o melhor desempenho do DeepSeek-R1, siga estas recomenda√ß√µes oficiais:

### Par√¢metros de Gera√ß√£o

| Par√¢metro | Valor Recomendado | Descri√ß√£o |
|-----------|-------------------|-----------|
| `temperature` | 0.5 - 0.7 (ideal: **0.6**) | Evita repeti√ß√µes infinitas e sa√≠das incoerentes |
| `top_p` | 0.95 | Controle de nucleus sampling |
| `max_tokens` | 32768 | Limite m√°ximo de gera√ß√£o |

### Melhores Pr√°ticas

> [!IMPORTANT]
> **N√ÉO use system prompts!** Todas as instru√ß√µes devem estar no prompt do usu√°rio.

> [!TIP]
> Para problemas matem√°ticos, inclua no prompt:  
> *"Please reason step by step, and put your final answer within \boxed{}."*

> [!WARNING]
> O modelo pode pular o padr√£o de pensamento (outputando `<think>\n\n</think>`). Para for√ßar o racioc√≠nio completo, **inicie a resposta com `<think>\n`**.

### Forcing Thinking Mode

Para garantir que o modelo sempre use o modo de racioc√≠nio:

```python
# Force o modelo a iniciar com <think>
response = client.chat.completions.create(
    model="deepseek-reasoner",
    messages=[
        {"role": "user", "content": "Resolva: x¬≤ + 2x - 3 = 0"}
    ],
    temperature=0.6,
    # Alguns SDKs permitem prefixar a resposta:
    # prefix="<think>\n"
)
```

### Exemplo de Prompt Otimizado

```markdown
Resolva o seguinte problema passo a passo, mostrando todo seu racioc√≠nio.
Coloque sua resposta final dentro de \boxed{}.

Problema: Um trem viaja a 80 km/h. Quanto tempo leva para percorrer 320 km?
```

---

## üìö Exemplos de Uso

### Exemplo 1: Resolu√ß√£o de Problemas Matem√°ticos

```
Prompt: Resolva passo a passo: Se f(x) = 3x¬≤ - 2x + 1, qual √© f'(x)?

Resposta esperada:
<think>
Para encontrar a derivada f'(x), vou aplicar a regra da pot√™ncia:
- A derivada de ax^n √© n*ax^(n-1)

Aplicando:
- 3x¬≤ ‚Üí 2 * 3x^(2-1) = 6x
- -2x ‚Üí 1 * (-2)x^(1-1) = -2
- 1 ‚Üí 0 (constante)

Somando: f'(x) = 6x - 2
</think>

Portanto, \boxed{f'(x) = 6x - 2}
```

### Exemplo 2: An√°lise de C√≥digo

```
Prompt: Analise este c√≥digo Python e identifique poss√≠veis problemas:

def calcular_media(numeros):
    soma = 0
    for n in numeros:
        soma += n
    return soma / len(numeros)

Resposta: O c√≥digo tem um problema potencial - divis√£o por zero caso 
a lista esteja vazia. Solu√ß√£o:

def calcular_media(numeros):
    if not numeros:
        raise ValueError("Lista vazia")
    return sum(numeros) / len(numeros)
```

### Exemplo 3: Racioc√≠nio L√≥gico

```
Prompt: Se todos os gatos s√£o mam√≠feros, e alguns mam√≠feros s√£o animais 
dom√©sticos, podemos concluir que todos os gatos s√£o animais dom√©sticos?

Resposta:
<think>
Premissa 1: Todos os gatos s√£o mam√≠feros (Gatos ‚äÇ Mam√≠feros)
Premissa 2: Alguns mam√≠feros s√£o animais dom√©sticos (Mam√≠feros ‚à© Dom√©sticos ‚â† ‚àÖ)

A premissa 2 diz que ALGUNS mam√≠feros s√£o dom√©sticos, n√£o TODOS.
Os gatos podem ou n√£o estar na interse√ß√£o Mam√≠feros ‚à© Dom√©sticos.
</think>

N√£o, n√£o podemos concluir isso. A premissa indica apenas que existe 
interse√ß√£o entre mam√≠feros e animais dom√©sticos, mas n√£o que todos 
os subconjuntos de mam√≠feros (como gatos) est√£o nessa interse√ß√£o.
```

### Exemplo 4: Upload de Arquivos (via API/Web)

```python
# Template para an√°lise de arquivos
file_template = """
[file name]: {file_name}
[file content begin]
{file_content}
[file content end]

{question}
"""

prompt = file_template.format(
    file_name="relatorio.csv",
    file_content="nome,vendas\nJo√£o,100\nMaria,150\nPedro,80",
    question="Qual vendedor teve mais vendas?"
)
```

---

## üí∞ Pre√ßos da API

Os pre√ßos s√£o por **1 milh√£o de tokens**:

| Recurso | Descri√ß√£o |
|---------|-----------|
| **Token** | Menor unidade de texto (palavra, n√∫mero, pontua√ß√£o) |
| **Cobran√ßa** | Total de tokens de entrada + sa√≠da |

### Regras de Dedu√ß√£o

- Despesa = N√∫mero de tokens √ó Pre√ßo
- Saldo de b√¥nus √© usado primeiro (quando dispon√≠vel)
- Pre√ßos podem variar - consulte [pricing](https://api-docs.deepseek.com/quick_start/pricing)

> üí° **Dica:** O DeepSeek oferece pre√ßos muito competitivos comparado a concorrentes!

### Contato Comercial

- üìß Email: api-service@deepseek.com
- üí¨ Discord: [discord.gg/Tc7c45Zzu5](https://discord.gg/Tc7c45Zzu5)
- üê¶ Twitter: [@deepseek_ai](https://twitter.com/deepseek_ai)

---

## üîç Troubleshooting

### Problemas Comuns

#### ‚ùå "Out of Memory" (GPU)

**Solu√ß√£o 1:** Use um modelo menor
```bash
ollama run deepseek-r1:1.5b  # Modelo mais leve
```

**Solu√ß√£o 2:** Habilite quantiza√ß√£o
```bash
# vLLM com quantiza√ß√£o
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \
    --quantization awq
```

**Solu√ß√£o 3:** Reduza o contexto
```bash
vllm serve model --max-model-len 8192  # Reduzir de 32768
```

#### ‚ùå Modelo gerando respostas incoerentes/repetitivas

**Solu√ß√£o:** Ajuste a temperatura:
```python
temperature = 0.6  # Valor ideal recomendado oficialmente
```

#### ‚ùå Modelo pulando o processo de racioc√≠nio

**Solu√ß√£o:** Force o in√≠cio do racioc√≠nio incluindo o prefixo na resposta:
```python
# Seu prompt deve esperar que o modelo comece com <think>
```

#### ‚ùå Erro "CUDA not available" (vLLM)

**Solu√ß√£o:** 
1. Verifique se voc√™ tem uma GPU NVIDIA
2. Instale os drivers CUDA: [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)
3. Reinstale o PyTorch com suporte CUDA:
```bash
uv pip install vllm --torch-backend=cu126
```

#### ‚ùå "Hugging Face's Transformers has not been directly supported yet"

**Nota:** Para DeepSeek-R1/V3 completo, use vLLM, SGLang, LMDeploy ou TRT-LLM. Transformers direto ainda n√£o √© suportado.

Os modelos **Distill** funcionam normalmente com Transformers:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
)
```

#### ‚ùå Download lento do modelo

**Solu√ß√µes:** 
- Use conex√£o cabeada
- Desative VPN se estiver usando
- Use espelho do HuggingFace:
```bash
export HF_ENDPOINT="https://hf-mirror.com"
```

#### ‚ùå vLLM n√£o funciona no Windows

**Nota:** vLLM requer Linux. Alternativas para Windows:
1. Use WSL2 (Windows Subsystem for Linux)
2. Use Ollama (tem suporte nativo a Windows)
3. Use Docker

---

## üìñ Recursos Adicionais

### Links Oficiais DeepSeek

| Recurso | Link |
|---------|------|
| üìÑ Paper DeepSeek-R1 | [arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948) |
| üìÑ Paper DeepSeek-V3 | [arxiv.org/pdf/2412.19437](https://arxiv.org/pdf/2412.19437) |
| üíª GitHub DeepSeek-R1 | [github.com/deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) |
| üíª GitHub DeepSeek-V3 | [github.com/deepseek-ai/DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) |
| ü§ó HuggingFace | [huggingface.co/deepseek-ai](https://huggingface.co/deepseek-ai) |
| üí¨ Chat Online | [chat.deepseek.com](https://chat.deepseek.com) |
| üîå API Platform | [platform.deepseek.com](https://platform.deepseek.com) |
| üìö API Docs | [api-docs.deepseek.com](https://api-docs.deepseek.com/) |

### Links vLLM

| Recurso | Link |
|---------|------|
| üìö Documenta√ß√£o | [docs.vllm.ai](https://docs.vllm.ai) |
| üíª GitHub | [github.com/vllm-project/vllm](https://github.com/vllm-project/vllm) |
| üìù Blog | [blog.vllm.ai](https://blog.vllm.ai/) |
| üìÑ Paper | [arxiv.org/abs/2309.06180](https://arxiv.org/abs/2309.06180) |
| üó∫Ô∏è Roadmap | [roadmap.vllm.ai](https://roadmap.vllm.ai) |
| üí¨ Slack | [slack.vllm.ai](https://slack.vllm.ai) |
| üí¨ Forum | [discuss.vllm.ai](https://discuss.vllm.ai) |

### Outros Frameworks

| Framework | GitHub | Documenta√ß√£o |
|-----------|--------|--------------|
| SGLang | [sgl-project/sglang](https://github.com/sgl-project/sglang) | - |
| LMDeploy | [InternLM/lmdeploy](https://github.com/InternLM/lmdeploy) | - |
| TensorRT-LLM | [NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) | - |
| Ollama | [ollama/ollama](https://github.com/ollama/ollama) | [ollama.com](https://ollama.com) |

### HuggingFace

| Recurso | Link |
|---------|------|
| üìö Documenta√ß√£o | [huggingface.co/docs](https://huggingface.co/docs) |
| ü§ó Hub | [huggingface.co](https://huggingface.co) |

---

## üìä Benchmarks de Desempenho

O DeepSeek-R1 foi avaliado com:
- **Temperature:** 0.6
- **Top-p:** 0.95
- **Max Tokens:** 32,768
- **Amostras por query:** 64

### Resultados Destacados

- **DeepSeek-R1** atinge performance compar√°vel ao **OpenAI-o1** em math, code e reasoning
- **DeepSeek-R1-Distill-Qwen-32B** supera **OpenAI-o1-mini** em diversos benchmarks
- Novos resultados state-of-the-art para modelos densos (non-MoE)

---

## üìú Cita√ß√µes

### DeepSeek-R1

```bibtex
@misc{deepseekai2025deepseekr1,
    title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
    author={DeepSeek-AI},
    year={2025},
    eprint={2501.12948},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2501.12948}
}
```

### vLLM

```bibtex
@inproceedings{kwon2023efficient,
    title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
    author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and 
            Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
    booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
    year={2023}
}
```

---

## üìã Licenciamento

| Modelo | Licen√ßa Base |
|--------|-------------|
| DeepSeek-R1 / R1-Zero | MIT License |
| DeepSeek-V3 | MIT License |
| Distill-Qwen-* | Apache 2.0 (Qwen) + MIT (fine-tuning) |
| Distill-Llama-8B | Llama 3.1 License |
| Distill-Llama-70B | Llama 3.3 License |
| vLLM | Apache 2.0 |

### Contato DeepSeek

- üìß Email: service@deepseek.com
- üìß API: api-service@deepseek.com
- üêõ Issues: [GitHub Issues](https://github.com/deepseek-ai/DeepSeek-R1/issues)
- üí¨ Discord: [discord.gg/Tc7c45Zzu5](https://discord.gg/Tc7c45Zzu5)

---

## üéâ Conclus√£o

O DeepSeek-R1 representa um avan√ßo significativo em modelos de racioc√≠nio open-source. Com m√∫ltiplas op√ß√µes de tamanho e m√©todos de instala√ß√£o, √© acess√≠vel desde usu√°rios com hardware modesto at√© data centers empresariais.

### Recomenda√ß√µes por Caso de Uso

| Caso de Uso | Recomenda√ß√£o |
|-------------|--------------|
| **Iniciantes/Testes** | Ollama + `deepseek-r1:7b` |
| **Desenvolvimento Local** | vLLM + `DeepSeek-R1-Distill-Qwen-14B` |
| **Produ√ß√£o** | vLLM/SGLang + `DeepSeek-R1-Distill-Qwen-32B` |
| **M√°xima Qualidade** | API DeepSeek + `deepseek-reasoner` |
| **Sem Instala√ß√£o** | [chat.deepseek.com](https://chat.deepseek.com) |

---

*Documenta√ß√£o atualizada em Dezembro 2024. Verifique os reposit√≥rios oficiais para informa√ß√µes mais recentes.*
